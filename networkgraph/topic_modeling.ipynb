{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "import hickle \n",
    "import gzip\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import pprint as pp\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "# bokeh\n",
    "import bokeh.plotting as bkplt\n",
    "\n",
    "\n",
    "# import requirments \n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import rpy2\n",
    "%load_ext rpy2.ipython\n",
    "%R require(\"ggplot2\")\n",
    "% matplotlib inline\n",
    "from ggplot import *\n",
    "randn = np.random.randn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "The first step was to remove retweets so the topics were not obscured by repeated tweets.\n",
    "\n",
    "The second stop was to make use of the NLP enrichments. The parts of speech in the NLP enrichment allowed us to __extract the nouns__. This extraction process is detailed in `organize_data.py`.\n",
    "\n",
    "In summary, retweets were removed and nouns were used in the vocabulary for the topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter noun set\n",
    "Notice that we have many nouns (exact count above) that are potential features. We'll need to refine this number of features to focus on those dimensions that are most meaningful. \n",
    "\n",
    "At least two options exist:\n",
    "1.  We can create an ordered list of the nouns frequencies, graph them, and then remove the values outside of our desired thresholds. \n",
    "2.  We can create a list of tweets that contain only nouns, vectorize them with TFIDF (and add bigrams). \n",
    "\n",
    "We'll play with option #1 and then focus on option #2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# organize nouns counts from largest to smallest\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# convert data to pandas df\n",
    "df = pd.DataFrame(sorted_counts, columns=('noun','counts'))\n",
    "\n",
    "# build the initial count for the summing process (we'll build a cdf)\n",
    "cdf = sorted_counts[0][1]\n",
    "\n",
    "# total of all counts\n",
    "total = df.counts.sum()\n",
    "\n",
    "# list w/ initial element [(term, count, summed_count, percentile, delta)]\n",
    "cdf_data = [(sorted_counts[0][0]\n",
    "             , sorted_counts[0][1]\n",
    "             , sorted_counts[0][1]\n",
    "             , 1-(sorted_counts[0][1]*1.0/total)\n",
    "             , 0 )]\n",
    "\n",
    "# sum up totals (start at 1; do not start at 0)\n",
    "for i in range(1,len(sorted_counts)):\n",
    "    prev_cdf = cdf\n",
    "    cdf += sorted_counts[i][1]\n",
    "    # list containing cdf info [(term, count, summed_count, percentile, delta)]\n",
    "    cdf_data.append((sorted_counts[i][0]\n",
    "                     , sorted_counts[i][1]\n",
    "                     , cdf\n",
    "                     , 1- (cdf*1.0/total)\n",
    "                     , cdf-prev_cdf ))\n",
    "\n",
    "# view cdf results\n",
    "display(cdf_data[:20])\n",
    "\n",
    "# create dataframe from the list cdf_data\n",
    "cdf_data_df = pd.DataFrame(cdf_data,columns=('noun','count','sum','percentile', 'delta'))\n",
    "\n",
    "# bucket counts for specific percentiles defined in the range\n",
    "pL = []\n",
    "percentile = []\n",
    "for i in range(50,100,5):\n",
    "    pL.append(len(cdf_data_df[cdf_data_df.percentile>(1-(i/100.0))]))\n",
    "    percentile.append(str(i))\n",
    "\n",
    "\n",
    "# create dataframe for plotting\n",
    "plot_data = pd.DataFrame(\n",
    "    {'sumcounts': pL\n",
    "     , 'percentiles': percentile\n",
    "    }\n",
    ")\n",
    "\n",
    "# show total counts\n",
    "display(plot_data.head())\n",
    "print 'Total nouns: {:,}'.format(len(counts))\n",
    "\n",
    "# send the result to R\n",
    "%Rpush plot_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the cumulative distribution function, we can consider the frequency of the nouns and reduce the current 677K dimensions. The plot below suggests that the majority of the noun counts are above the 85th percentile. Exploring this data a little, we see that the few nouns in the upper percentils contain the majority of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#ggplot(data=plot_40, aes(x=percentile,y=sum))+geom_bar(stat=\"identity\")+ggtitle(\"CDF: Noun Counts\")\n",
    "ggplot(data=plot_data, aes(x=percentiles,y=sumcounts))+geom_bar(stat=\"identity\")+coord_flip()+ggtitle(\"CDF: Noun Counts\")+xlab('Percentile (What % of nouns are below this set?)')+ylab('Set Size (Counts associated w/ Nouns in this set.)')\n",
    "#head(plot_50)\n",
    "#summary(plot_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So under option #1, we can use the graph above to inform a method to restrict the features in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduced_vocab = cdf_data_df[cdf_data_df.percentile>0.40]\n",
    "display(reduced_vocab.head())\n",
    "print 'Nouns to use in Option1 vocabulary: {:,}'.format(len(reduced_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write reduced vocab\n",
    "reduced_vocab.to_pickle('reduced_vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vectorizers \n",
    "Option1: We'll use the newly reduced vocabulary to build a vectorizer. (previously completed)  \n",
    "Option2: We'll use the vectorizer's TFIDF utility to build a vocabulary set. (completed below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "def vectorize_1(vocab):\n",
    "    vectorizer = TfidfVectorizer(#min_df=20\n",
    "                                 stop_words='english'\n",
    "                                 , use_idf=True # enable inverse-document-frequency reweighting\n",
    "                                 , ngram_range=(1,2) # given our vocab, not really necessary\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 , vocabulary = vocab\n",
    "                                ) \n",
    "    #X = vectorizer.fit_transform(tweet_list)\n",
    "    return vectorizer\n",
    "\n",
    "def vectorize_2():\n",
    "    vectorizer = CountVectorizer(min_df=20\n",
    "                                 , stop_words='english'\n",
    "                                 , ngram_range=(1,2) # given our vocab, not really necessary\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 #, vocabulary = set(vocab)\n",
    "                                ) \n",
    "    #X = vectorizer.fit_transform(tweet_list)\n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the vectorizers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the vectorizer for option1\n",
    "red_vectorizer = vectorize_1(reduced_vocab.noun)\n",
    "# Build the vectorizer for option2\n",
    "noun_vectorizer = vectorize_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing/Training Data\n",
    "\n",
    "We'll apply the vectorizer function that we built earlier, form the doc term matrix on the training set, and develop the topic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_here = False\n",
    "\n",
    "def create_index(total_tweets):\n",
    "        \"\"\"\n",
    "        Builds an index for the training and test set.\n",
    "        The sets serve as a list of row numbers to extract from the dataset. \n",
    "        \"\"\"\n",
    "        # based on the total tweet count, create an array of all line numbers \n",
    "        line_index = np.array(range(0,total_tweets))\n",
    "        line_index_nouns = np.array(range(0,len(all_tweets_nouns_only)))\n",
    "        # split the array into training and test sets of index values\n",
    "        trainIndex,testIndex = train_test_split(line_index,train_size=0.20, random_state=42)\n",
    "        trainIndex_nouns,testIndex_nouns = train_test_split(line_index_nouns,train_size=0.20, random_state=42)\n",
    "        # save test & traning index values\n",
    "        np.save(\"training_index\",trainIndex)\n",
    "        np.save(\"testing_index\",testIndex)\n",
    "        return trainIndex,testIndex, trainIndex_nouns,testIndex_nouns\n",
    "\n",
    "if run_here:\n",
    "    trainIndex,testIndex, trainIndex_nouns,testIndex_nouns = create_index(total_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the text for the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_txt_data(index_set, infile, out_file_path='_data.pkl',write_to_file=False, json_format=False):\n",
    "    \"\"\"\n",
    "    Reads the infile and uses the appropriate index to return the training/test data.\n",
    "    \"\"\"\n",
    "    if isinstance(infile,list):\n",
    "        txt = [infile[i] for i in index_set]\n",
    "    elif not json:\n",
    "        with gzip.open(infile,'rb') as body_file:\n",
    "            # read all lines into memory and form a list\n",
    "            body_list = body_file.readlines()\n",
    "            txt = [body_list[i] for i in index_set]\n",
    "    else:\n",
    "        with gzip.open(infile,'rb') as body_file:\n",
    "            # read each line, parse json, and form a list\n",
    "            txt = []\n",
    "            index_lookup = dict([(i,0) for i in index_set])\n",
    "            for i,line in enumerate(body_file):\n",
    "                #print line\n",
    "                #print (i not in index_lookup)\n",
    "                if i not in index_lookup:\n",
    "                    continue\n",
    "                rec = json.loads(line)\n",
    "                txt.append(rec['body']) \n",
    "            \n",
    "    if write_to_file:\n",
    "        with open(out_file_path,'wb') as f:\n",
    "            # save txt\n",
    "            pickle.dump(txt, f)\n",
    "            \n",
    "    return txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build the traning/test sets for each of the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Build model specific traning sets\n",
    "###\n",
    "run_here = False \n",
    "if run_here:\n",
    "    training_txt_list = get_txt_data(\n",
    "        trainIndex\n",
    "        , '<filePath>'\n",
    "        , 'training_txt.pkl'\n",
    "        , True\n",
    "        , True)\n",
    "\n",
    "    testing_txt_list = get_txt_data(\n",
    "        testIndex\n",
    "        , '<filePath>'\n",
    "        , 'testing_txt.pkl'\n",
    "        , True\n",
    "        , True)\n",
    "\n",
    "    training_noun_list = get_txt_data(\n",
    "        trainIndex_nouns\n",
    "        , all_tweets_nouns_only\n",
    "        , 'training_nouns_txt.pkl'\n",
    "        , True\n",
    "        , True)\n",
    "\n",
    "    testing_noun_list = get_txt_data(\n",
    "        testIndex_nouns\n",
    "        , all_tweets_nouns_only\n",
    "        , 'testing_nouns_txt.pkl'\n",
    "        , True\n",
    "        , True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this data to disk, we won't need to run this section each time we open the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_here = False \n",
    "if run_here:\n",
    "    pickle.dump(training_txt_list,open('training_txt_list','wb'))\n",
    "    pickle.dump(testing_txt_list,open('testing_txt_list','wb'))\n",
    "    pickle.dump(training_noun_list,open('training_noun_list','wb'))\n",
    "    pickle.dump(testing_noun_list,open('testing_noun_list','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_here = True\n",
    "if run_here:\n",
    "    training_txt_list = pickle.load(open('training_txt_list','rb'))\n",
    "    testing_txt_list = pickle.load(open('testing_txt_list','rb'))\n",
    "    training_noun_list = pickle.load(open('training_noun_list','rb'))\n",
    "    testing_noun_list = pickle.load(open('testing_noun_list','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"manual reduction method\"\n",
    "print \"  len(training_txt_list): {:,} | len(testing_txt_list): {:,}.\".format(len(training_txt_list),len(testing_txt_list))\n",
    "print \"Nouns w/ bigrams method:\"\n",
    "print \"  len(training_txt_list): {:,} | len(testing_txt_list): {:,}.\".format(len(training_noun_list),len(testing_noun_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data\n",
    "We'll now apply our vectorizers to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize the tweets for option1 \n",
    "X_reduced = red_vectorizer.fit_transform(training_txt_list)\n",
    "\n",
    "# Vectorize the tweets for option2 \n",
    "X_noun = noun_vectorizer.fit_transform(training_noun_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(noun_vectorizer.get_feature_names(),open('noun_vectorizer_vocab','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"total vocab manual reduction: {:,} | total vocab TFIDF reduction w/ bigrams: {:,}\".format(len(red_vectorizer.get_feature_names()),len(noun_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction?\n",
    "We'll apply SVD to reduce the number of dimensions in a new space. We use kmeans to build 5 centroids in the data. These centroids will serve as the basis for our topic clusters. \n",
    "\n",
    "The first step is to decide how many components to use for SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent of explained variance\n",
    "Reducing the numbrer of components affects the amount of variance that we can explain. The graph below helped us determine a reasonable number of compnents to use when applying the topic model. Notice that almost 70% of the variance is explained using 300 components for the manual reduction but we need over 1000 components to explain the same amount of variance in the noun based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#explained_variances = np.var(X_svd, axis=0) / np.var(X_train, axis=0).sum()\n",
    "\n",
    "### \n",
    "### This can take 10-15 mins\n",
    "###\n",
    "def create_svd_doc_term_matrix(X_train, num_eigen_vectors=100):\n",
    "    \"\"\"\n",
    "    Create the array with truncated svd.\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components = num_eigen_vectors)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "    return lsa.fit_transform(X_train), svd\n",
    "\n",
    "run_here = False\n",
    "if run_here:\n",
    "    explained_variance_red = []\n",
    "    explained_variance_nouns = []\n",
    "    svd_component_range = range(100,351,50)\n",
    "    for i in svd_component_range:\n",
    "        # find explained variance in manual reduction method\n",
    "        X_svd_red, svd_red = create_svd_doc_term_matrix(X_reduced,i)\n",
    "        explained_variance_red.append(svd_red.explained_variance_ratio_.sum())\n",
    "        # find explained variance in noun reduction method\n",
    "        X_svd_noun, svd_noun = create_svd_doc_term_matrix(X_noun,i*3)\n",
    "        explained_variance_nouns.append(svd_noun.explained_variance_ratio_.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data below, the idea is that we will use only 300 components for the manual reduction model but 1,050 components for the noun model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_here = False\n",
    "if run_here:\n",
    "    expVar = pd.DataFrame({'explained_red':explained_variance_red\n",
    "                       , 'explained_nouns':explained_variance_nouns\n",
    "                       , 'components_red':svd_component_range\n",
    "                       , 'components_nouns':[comp*3 for comp in svd_component_range] })\n",
    "    display(expVar)\n",
    "    #%Rpush  expVar\n",
    "    display(expVar.plot(x='components_red',y='explained_red'))\n",
    "    display(expVar.plot(x='components_nouns',y='explained_nouns'))\n",
    "    #list(expVar.components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SVD, we can __reduced__ the number of our dimensions even further. In the next section, we'll use this technique and build the cluster centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#p = bkplt.figure(title=\"Explained Variance per n Components\"\n",
    "#                 , x_axis_label='N Components'\n",
    "#                 , y_axis_label='Explained Variance')\n",
    "#p.circle(list(expVar.components), list(expVar.explained), legend=\"Variance Explained\", fill_color=\"red\", line_color=\"red\", size=6)\n",
    "#bkplt.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%R\n",
    "#print(expVar)\n",
    "#ggplot(data=expVar)+geom_bar(aes(x=components,y=explained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cluster centroids\n",
    "We'll now apply kmeans to find the centroids that will be used to predict a cluster for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_clusters(X_svd, k=5):\n",
    "    \"\"\"\n",
    "    Use kmeans to find centroids.\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=k\n",
    "                , init='k-means++'\n",
    "                , max_iter=100\n",
    "                #, n_init=10\n",
    "                , verbose=False)\n",
    "    km.fit(X_svd)\n",
    "    pred=km.predict(X_svd)\n",
    "    pred_df=pd.DataFrame(pred)\n",
    "    pred_df.columns=['pred_cluster']\n",
    "    return km.cluster_centers_ , pred_df, k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_here = False\n",
    "if run_here:\n",
    "    centroids_noun, predictions_noun, n_clusters_noun = build_clusters(X_svd_noun, 5)\n",
    "    centroids_red, predictions_red, n_clusters_red = build_clusters(X_svd_red, 5)\n",
    "    centroids_noun50, predictions_noun50, n_clusters_noun50 = build_clusters(X_svd_noun, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "run_here = False\n",
    "if run_here:\n",
    "    pickle.dump(centroids_noun, open('centroids_noun','wb'))\n",
    "    pickle.dump(predictions_noun, open('predictions_noun','wb'))\n",
    "\n",
    "    pickle.dump(centroids_red, open('centroids_red','wb'))\n",
    "    pickle.dump(predictions_red, open('predictions_red','wb'))\n",
    "\n",
    "    pickle.dump(centroids_noun50, open('centroids_noun50','wb'))\n",
    "    pickle.dump(predictions_noun50, open('predictions_noun50','wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids_noun = pickle.load(open('centroids_noun','rb'))\n",
    "predictions_noun = pickle.load(open('predictions_noun','rb'))\n",
    "\n",
    "centroids_red = pickle.load(open('centroids_red','rb'))\n",
    "predictions_red = pickle.load(open('predictions_red','rb'))\n",
    "\n",
    "centroids_noun50 = pickle.load(open('centroids_noun50','rb'))\n",
    "predictions_noun50 = pickle.load(open('predictions_noun50','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the clusters as topics?\n",
    "We can look at the top words loaded onto each cluster to consider a human readible forms. To understand the vocabulary used in the svd space, we need to transform from svd space to the original dimensions, which also provides the word loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_here = False\n",
    "if run_here:\n",
    "    word_loadings_red=np.dot(centroids_red, svd_red.components_)\n",
    "    pickle.dump(word_loadings_red, open('word_loadings_red','wb'))\n",
    "\n",
    "    word_loadings_noun=np.dot(centroids_noun, svd_noun.components_)\n",
    "    pickle.dump(word_loadings_noun, open('word_loadings_noun','wb'))\n",
    "\n",
    "    word_loadings_noun50=np.dot(centroids_noun50, svd_noun.components_)\n",
    "    pickle.dump(word_loadings_noun50, open('word_loadings_noun50','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_loadings_red=pickle.load(open('word_loadings_red','rb'))\n",
    "#np.save('word_loadings_red',word_loadings_red)\n",
    "#word_loadings_red = np.load('./word_loadings_red.npy')\n",
    "#word_loadings=np.dot(centroids, svd.components_)\n",
    "#print(word_loadings.shape)\n",
    "#vocab=vectorizer.get_feature_names()\n",
    "#for k in range(0,n_clusters):\n",
    "run_here = False\n",
    "if run_here:\n",
    "    for k in range(0,5):\n",
    "        #word loadings = cluster_centers * eigenvectors \n",
    "        indices=[i for i in np.argsort(word_loadings_red[k,:])[::-1]]    \n",
    "        sorted_vocab=[reduced_vocab.noun[i] for i in indices]\n",
    "        print(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_loadings_noun = pickle.load(open('word_loadings_noun','wb'))\n",
    "\n",
    "#np.save('word_loadings_noun',word_loadings_noun)\n",
    "#word_loadings_noun = np.load('./word_loadings_noun.npy')\n",
    "#word_loadings=np.dot(centroids, svd.components_)\n",
    "#print(word_loadings.shape)\n",
    "#vocab=vectorizer.get_feature_names()\n",
    "#for k in range(0,n_clusters):\n",
    "run_here = False\n",
    "if run_here:\n",
    "    for k in range(0,5):\n",
    "        #word loadings = cluster_centers * eigenvectors \n",
    "        indices=[i for i in np.argsort(word_loadings_noun[k,:])[::-1]]    \n",
    "        sorted_vocab=[noun_vectorizer.get_feature_names()[i] for i in indices]\n",
    "        print(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_loadings_noun50 = pickle.load(open('word_loadings_noun50','wb'))\n",
    "#np.save('word_loadings_noun50',word_loadings_noun50)\n",
    "#word_loadings_noun = np.load('./word_loadings_noun.npy')\n",
    "#word_loadings=np.dot(centroids, svd.components_)\n",
    "#print(word_loadings.shape)\n",
    "#vocab=vectorizer.get_feature_names()\n",
    "#for k in range(0,n_clusters):\n",
    "run_here = False\n",
    "if run_here:  \n",
    "    for k in range(0,50):\n",
    "        #word loadings = cluster_centers * eigenvectors \n",
    "        indices=[i for i in np.argsort(word_loadings_noun50[k,:])[::-1]]    \n",
    "        sorted_vocab=[noun_vectorizer.get_feature_names()[i] for i in indices]\n",
    "        print(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "We'll now use these cluster centers to consider the test tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_tweets(vectorizer, word_loadings, testing_data, sample_percentage=0.20):\n",
    "    \"\"\"\n",
    "    Label tweets.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    sample_size = int(len(testing_data)*sample_percentage)\n",
    "    sample_tweets = testing_data[:sample_size]\n",
    "    for tweet in sample_tweets:\n",
    "        # vectorize the tweet\n",
    "        sparse_array = vectorizer.fit_transform([tweet])\n",
    "        # subtract all values between the tweet vectorization and centroids\n",
    "        sparse_array_subtraction_abs = np.absolute(sparse_array - word_loadings)\n",
    "        # sum to get the total distances \n",
    "        sparse_array_subtraction_abs_sum = sparse_array_subtraction_abs.sum(axis=1)\n",
    "        # append the index of the minimum distance\n",
    "        result.append(np.argmin(sparse_array_subtraction_abs_sum))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_tweets(vectorized_tweet, word_loadings):\n",
    "    \"\"\"\n",
    "    Label tweets.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # vectorize the tweet\n",
    "    sparse_array = vectorized_tweet\n",
    "    # subtract all values between the tweet vectorization and centroids\n",
    "    sparse_array_subtraction_abs = np.absolute(sparse_array - word_loadings)\n",
    "    # sum to get the total distances \n",
    "    sparse_array_subtraction_abs_sum = sparse_array_subtraction_abs.sum(axis=1)\n",
    "    # append the index of the minimum distance\n",
    "    result.append(np.argmin(sparse_array_subtraction_abs_sum))\n",
    "    myArray = sparse_array_subtraction_abs_sum\n",
    "    return result,myArray\n",
    "\n",
    "def vectorize_data(tweet_list,custom_vocab=False, vocab=[]):\n",
    "    \"\"\"\n",
    "    Use the TFidfVectorizer or CountVectorizer to vectorize a set of tweets. \n",
    "    \"\"\"\n",
    "    sys.stdout.write('Vectorizing the tweets.'+'\\n')\n",
    "    #vectorizer = TfidfVectorizer(min_df=20\n",
    "    vectorizer = CountVectorizer(#min_df=20\n",
    "                                 #, use_idf = True\n",
    "                                 stop_words='english'\n",
    "                                 , ngram_range=(1,2)\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 , vocabulary = vocab\n",
    "                                ) \n",
    "    X = vectorizer.fit_transform(tweet_list)\n",
    "    if len(vocab)==0:\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "    sys.stdout.write('  - len(vocab):{:,}\\n'.format(len(vocab)))\n",
    "    return X\n",
    "\n",
    "craft45_word_loadings = hickle.load('../enrichments/models/word_loadings50_bigrams_v1.hkl')\n",
    "craft45_vocab = pickle.load(open('../enrichments/models/vocab_bigrams_v1.pkl'))\n",
    "\n",
    "label,myArray = label_tweets(vectorize_data(['I like beer'],True, craft45_vocab),craft45_word_loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(np.min(myArray), np.std(myArray), np.max(myArray), np.mean(myArray))\n",
    "display(myArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "red_result = label_tweets(red_vectorizer, word_loadings_red, testing_txt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(red_result,open('red_result','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun_vocab = pickle.load(open('noun_vectorizer_vocab','rb'))\n",
    "noun_vectorizer.set_params(min_df = 0.1, vocabulary = noun_vocab)\n",
    "noun_result = label_tweets(noun_vectorizer, word_loadings_noun, testing_noun_list)\n",
    "pickle.dump(noun_result,open('noun_result','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun_vectorizer.set_params(min_df = 0.1, vocabulary = noun_vocab)\n",
    "noun50_result = label_tweets(noun_vectorizer, word_loadings_noun50, testing_noun_list)\n",
    "pickle.dump(noun50_result,open('noun_result50','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate (shorter cycle)\n",
    "Run the entire process in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! gzip -cd /Users/blehman/... | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_body = '/Users/blehman/...'\n",
    "tweet_body = '/Users/blehman/...'\n",
    "with gzip.open(tweet_body, 'rb') as tweet_set:\n",
    "    print len(tweet_set.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_body = '/Users/blehman...'\n",
    "sum([1 for line in gzip.open(tweet_body, 'rb')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "tweet_body = '/Users/blehman/...'\n",
    "os.path.getsize(tweet_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open ('test','wb') as f:\n",
    "    x = [1,2,3]\n",
    "    for line in x:\n",
    "        f.write(str(line)+'\\n')\n",
    "print os.path.getsize('test')\n",
    "with open ('test','rb') as f:\n",
    "    print f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_index(tweet_list, set_name, training_percent=0.20,write_to_file=False):\n",
    "        \"\"\"\n",
    "        Builds an index for the training and test set.\n",
    "        The sets serve as a list of row numbers to extract from the dataset. \n",
    "        \"\"\"\n",
    "        # get file len\n",
    "        file_len = len(tweet_list)\n",
    "        # based on the total tweet count, create an array of all line numbers \n",
    "        file_len_array = np.array(range(0,file_len))\n",
    "        \n",
    "        # split the array into training and test sets of index values\n",
    "        sys.stdout.write('Build training/test sets.' + '\\n')\n",
    "        train_index,test_index = train_test_split(file_len_array,train_size=training_percent, random_state=42)\n",
    "        \n",
    "        # save test & traning index values\n",
    "        if write_to_file:\n",
    "            sys.stdout.write('  - writing training/test sets to file.' + '\\n')\n",
    "            hickle.dump(train_index, \"training_index_\"+set_name + \".hkl\", mode='w', compression='gzip')\n",
    "            hickle.dump(test_index, \"testing_index_\"+set_name + \".hkl\", mode='w', compression='gzip')\n",
    "        print train_index\n",
    "        return train_index,test_index\n",
    "\n",
    "def vectorize_data(tweet_list,custom_vocab=False, vocab=[]):\n",
    "    \"\"\"\n",
    "    Use the TFidfVectorizer or CountVectorizer to vectorize a set of tweets. \n",
    "    \"\"\"\n",
    "    sys.stdout.write('Vectorizing the tweets.'+'\\n')\n",
    "    #vectorizer = TfidfVectorizer(min_df=20\n",
    "    vectorizer = CountVectorizer(min_df=20\n",
    "                                 #, use_idf = True\n",
    "                                 , stop_words='english'\n",
    "                                 , ngram_range=(1,2)\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 #, vocabulary = set(vocab)\n",
    "                                ) \n",
    "    X = vectorizer.fit_transform(tweet_list)\n",
    "    if len(vocab)==0:\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "    sys.stdout.write('  - len(vocab):{:,}\\n'.format(len(vocab)))\n",
    "    return X, vocab\n",
    "\n",
    "\n",
    "\n",
    "def get_data(tweet_list,index_):\n",
    "    sys.stdout.write('Getting specific tweets for training/test.' + '\\n')\n",
    "    data = [tweet_list[i] for i in index_]\n",
    "    return data\n",
    "    \n",
    "\n",
    "def create_svd_doc_term_matrix(X_train, num_eigen_vectors=100):\n",
    "    \"\"\"\n",
    "    Create the array with truncated svd.\n",
    "    \"\"\"\n",
    "    sys.stdout.write('Using SVD w/ {:,} components'.format(num_eigen_vectors) + '\\n')\n",
    "    svd = TruncatedSVD(n_components = num_eigen_vectors)\n",
    "    pipeline = make_pipeline(svd, Normalizer(copy=False))\n",
    "    return pipeline.fit_transform(X_train), svd\n",
    "\n",
    "def view_explained_variance(X_train,svd_component_range = range(100,351,50)):\n",
    "    \"\"\"\n",
    "    Builds graphs to determine an appropriate number of SVD components to use w/ kmeans.\n",
    "    \"\"\"\n",
    "    explained_variance = []\n",
    "\n",
    "    # build list of explained variance values\n",
    "    sys.stdout.write('Testing w/ {} components.'.format(svd_component_range) + '\\n')\n",
    "    for i in svd_component_range:\n",
    "        # find explained variance \n",
    "        X_svd, svd = create_svd_doc_term_matrix(X_train,i)\n",
    "        exp_var = svd.explained_variance_ratio_.sum()\n",
    "        print \"  - Explained variance w/ {} components: {}\".format(i,exp_var)\n",
    "        explained_variance.append(exp_var)\n",
    "    \n",
    "    # graph values\n",
    "    expVar = pd.DataFrame({'components':svd_component_range\n",
    "                           ,'explained_variance':explained_variance\n",
    "                          })\n",
    "    display(expVar)\n",
    "    #%Rpush  expVar\n",
    "    display(expVar.plot(x='components',y='explained_variance'))\n",
    "    #list(expVar.components)\n",
    "\n",
    "\n",
    "def build_clusters(X_svd, k=5):\n",
    "    \"\"\"\n",
    "    Use kmeans to find centroids.\n",
    "    \"\"\"\n",
    "    sys.stdout.write('Building clusters w/ k={}.'.format(k) + '\\n')\n",
    "    km = KMeans(n_clusters=k\n",
    "                , init='k-means++'\n",
    "                , max_iter=100\n",
    "                #, n_init=10\n",
    "                , verbose=False)\n",
    "    km.fit(X_svd)\n",
    "    pred=km.predict(X_svd)\n",
    "    pred_df=pd.DataFrame(pred)\n",
    "    pred_df.columns=['pred_cluster']\n",
    "    return km.cluster_centers_ , pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# config \n",
    "gziped_tweet_bodies = '/Users/blehman/...'\n",
    "training_set_size = 0.99 # percentage of data to use as training to build centroids\n",
    "load_prev_saved_training_set = False\n",
    "tweet_set_label = '_50cluster'\n",
    "num_clusters = 50\n",
    "\n",
    "for iteration in range(0,3):\n",
    "    # run process\n",
    "    with gzip.open(gziped_tweet_bodies, 'rb') as tweet_file_obj:\n",
    "        tweet_list = tweet_file_obj.readlines()\n",
    "        sys.stdout.write('')\n",
    "        if load_prev_saved_training_set:\n",
    "            # load train/test index values\n",
    "            train_index = hickle.load(\"training_index_\" + tweet_set_label + \".hkl\")\n",
    "            test_index = hickle.load(\"testing_index_\" + tweet_set_label + \".hkl\")\n",
    "        else: \n",
    "            # build train/test index values\n",
    "            train_index, test_index = create_index(tweet_list, tweet_set_label ,training_set_size, True)\n",
    "\n",
    "        # get specific tweets \n",
    "        X_train_txt_list = get_data(tweet_list, train_index)\n",
    "\n",
    "\n",
    "    #sys.stdout.write(\"len(X_train_txt_list):{:,} \\n\".format(len(X_train_txt_list)))\n",
    "    display(X_train_txt_list[:10])\n",
    "\n",
    "\n",
    "    # vectorize data\n",
    "    X_train_vect, vocab = vectorize_data(X_train_txt_list)\n",
    "\n",
    "    #(note: this step's runtime is 10-20 mins and can be skipped)\n",
    "    #view explained variance to select number of components to use w/ SVD \n",
    "    #view_explained_variance(X_train_vect,[3*x for x in range(100,351,50)])\n",
    "\n",
    "    # reduce dimensions with SVD\n",
    "    num_components = 600\n",
    "    X_train_svd, svd = create_svd_doc_term_matrix(X_train_vect, num_components)\n",
    "\n",
    "    # build clusters (note: this can take over 30 mins)\n",
    "    centroids, predictions = build_clusters(X_train_svd, num_clusters)\n",
    "\n",
    "    # save centroids and vocab\n",
    "    sys.stdout.write('Clusters Built. Saving data...'+'\\n')\n",
    "    hickle.dump(centroids, \"centroids5_\"+tweet_set_label + \"_bigrams_v\" + str(iteration) +\".hkl\", mode='w', compression='gzip')\n",
    "    \n",
    "    #hickle.dump(vocab, \"vocab_\"+tweet_set_label + \".hkl\", mode='w', compression='gzip')\n",
    "    pickle.dump(vocab, open(\"vocab_\"+tweet_set_label + \"_bigrams_vb\" + str(iteration) + \".pkl\",'wb'), protocol=2) \n",
    "    #pickle.dump(centroids, open(\"centroids_\"+tweet_set_label + \".pkl\",'wb'), protocol=2)\n",
    "    word_loadings = np.dot(centroids, svd.components_)\n",
    "    hickle.dump(word_loadings, \"word_loadings\" + str(num_clusters)+ \"_\" + tweet_set_label + \"_bigrams_vb\" + str(iteration) + \".hkl\", mode='w', compression='gzip')\n",
    "    \n",
    "    sys.stdout.write(\"Printing wordloadings {} to file. \\n\".format(word_loadings.shape))\n",
    "    with open(\"top_loaded_terms_\"+ str(num_clusters)+ \"_\"+ tweet_set_label + \"_bigrams_vb\" + str(iteration) +\".txt\", 'wb') as f:\n",
    "        for k in range(0,num_clusters):\n",
    "            #word loadings = cluster_centers * eigenvectors \n",
    "            indices = [i for i in np.argsort(word_loadings[k,:])[::-1]]    \n",
    "            sorted_vocab = [vocab[i] for i in indices]\n",
    "            f.write(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"top_loaded_terms\"+ str(num_clusters)+ \"_\" +tweet_set_label + \"_bigrams_vb\" + str(i) +\".txt\", 'wb') as f:\n",
    "    for k in range(0,5):\n",
    "            #word loadings = cluster_centers * eigenvectors \n",
    "            indices = [i for i in np.argsort(word_loadings[k,:])[::-1]]    \n",
    "            sorted_vocab = [vocab[i] for i in indices]\n",
    "            f.write(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"top_loaded_terms\"+ str(num_clusters)+ \"_\" +tweet_set_label + \"_bigrams_vb\" + str(i) +\".txt\", 'rb') as f:\n",
    "    for line in f:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wl = hickle.load('word_loadings5_beer_2012_5cluster_bigrams_vb0.hkl')\n",
    "wl = hickle.load('word_loadings5_beer_2012_5cluster_bigrams_vb0.hkl')\n",
    "for i in range(0,5):\n",
    "    wl = hickle.load('word_loadings5_beer_2012_5cluster_bigrams_vb'+str(i)+'.hkl')\n",
    "    for k in range(0,5):\n",
    "        #word loadings = cluster_centers * eigenvectors \n",
    "        indices = [i for i in np.argsort(wl[k,:])[::-1]]    \n",
    "        sorted_vocab = [vocab[i] for i in indices]\n",
    "        sys.stdout.write(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))\n",
    "    sys.stdout.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 200\n",
    "with open(\"top_loaded_terms\"+ str(num_clusters)+ \"_\" +tweet_set_label + \"_bigrams_v\" + str(i) +\".txt\", 'wb') as f:\n",
    "    for k in range(0,50):\n",
    "        #word loadings = cluster_centers * eigenvectors \n",
    "        indices = [i for i in np.argsort(word_loadings[k,:])[::-1]]    \n",
    "        sorted_vocab = [vocab[i] for i in indices]\n",
    "        f.write(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(vocab, open(\"vocab_\"+tweet_set_label + \"_BIGRAMS\" + \".pkl\",'wb'), protocol=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.stdout.write(\"Printing wordloadings {} to file. \\n\".format(word_loadings.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hickle.dump(word_loadings, \"word_loadings50_\" + tweet_set_label + \"_BIGRAMS.hkl\", mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hickle.dump(word_loadings, \"word_loadings50_\" + tweet_set_label + \"_BIGRAMS.hkl\", mode='w', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(Image(filename='/Users/blehman/Desktop/range1.png') )\n",
    "print \n",
    "print\n",
    "display(Image(filename='/Users/blehman/Desktop/range2.png') )\n",
    "print \n",
    "print \n",
    "display(Image(filename='/Users/blehman/Desktop/range3.png') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_model_dict['Craft45TopicModel'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.min(word_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array([1,1,1,1,0,200])*np.array([[0,0,0,1,0,5],[0,0,0,1,0,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_model_dict = {'FirstTopicEnrichment':( np.load('../enrichments/models/word_loadings.npy')\n",
    "                                            , np.load('../enrichments/models/vocab.npy'))\n",
    "                    , 'SecondTopicEnrichment':( np.load('../enrichments/models/word_loadings_v2.npy')\n",
    "                                               , np.load('../enrichments/models/vocab_v2.npy'))\n",
    "                    , 'ThirdTopicEnrichment':( pickle.load(open('../enrichments/models/word_loadings_noun','rb'))\n",
    "                                              , pickle.load(open('../enrichments/models/noun_vectorizer_vocab','rb')))\n",
    "\n",
    "                    , 'FourthTopicEnrichment':( pickle.load(open('../enrichments/models/word_loadings_noun50','rb'))\n",
    "                                               , pickle.load(open('../enrichments/models/noun_vectorizer_vocab','rb')))\n",
    "\n",
    "                    , 'TopicModelB':( hickle.load('../enrichments/models/word_loadings50_.hkl')\n",
    "                                            , pickle.load(open('../enrichments/models/vocab_.pkl','rb')))\n",
    "\n",
    "                    , 'TopicModel2': ( hickle.load('../enrichments/models/word_loadings50_bigrams.hkl')\n",
    "                                              , pickle.load(open('../enrichments/models/vocab_bigrams.pkl')))\n",
    "                    , 'C45TopicModel': ( hickle.load('../enrichments/models/word_loadings50_bigrams_v1.hkl')\n",
    "                                            , pickle.load(open('../enrichments/models/vocab_bigrams_v1.pkl')))\n",
    "                    , 'C45TopicModel_fix': ( hickle.load('../enrichments/models/word_loadings50_bigrams_v2.hkl')\n",
    "                                                , pickle.load(open('../enrichments/models/vocab_bigrams_v1.pkl')))\n",
    "                    , 'C42TopicModel': (hickle.load('../enrichments/models/word_loadings50_bigrams_vb4.hkl')\n",
    "                                            , pickle.load(open('../enrichments/models/vocab_bigrams_vb4.pkl')))\n",
    "                   }\n",
    "for topic_model_name,value in topic_model_dict.items():\n",
    "    word_loadings = value[0]\n",
    "    vocab = value[1]\n",
    "    with open(topic_model_name + '_top_wordloadings.txt','wb') as topic_model_file:\n",
    "        for k in range(0,word_loadings.shape[0]):\n",
    "            if k == 0:\n",
    "                topic_model_file.write(\"{}:\\n\".format(topic_model_name))\n",
    "            #word loadings = cluster_centers * eigenvectors \n",
    "            indices = [i for i in np.argsort(word_loadings[k,:])[::-1]]    \n",
    "            sorted_vocab = [vocab[i] for i in indices]\n",
    "            topic_model_file.write(\"  Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pickle.load(open('../enrichments/models/vocab_bigrams_vb4.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.stdout.write(\"Printing wordloadings {} to file. \\n\".format(word_loadings.shape))\n",
    "with open(\"top_loaded_terms_\"+tweet_set_label + \"_bigrams_v\" + str(iteration) +\".txt\", 'wb') as f:\n",
    "    for k in range(0,50):\n",
    "        #word loadings = cluster_centers * eigenvectors \n",
    "        indices = [i for i in np.argsort(word_loadings[k,:])[::-1]]    \n",
    "        sorted_vocab = [vocab[i] for i in indices]\n",
    "        f.write(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_here = False\n",
    "if run_here:\n",
    "    filePath = '<filePath>'\n",
    "    if os.path.isfile(filePath):\n",
    "        testing_data=np.load(filePath)\n",
    "    else:\n",
    "        testing_data = [json.loads(line)['body'] for line in get_txt_data(\n",
    "        np.load('testing.npy')\n",
    "        , '<filePath>'\n",
    "        , '<filePath>'\n",
    "        , True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check testing tweets\n",
    "#print \"total testing tweets: {:,}\".format(len(testing_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the model to our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_percentage=0.20\n",
    "sample_size = int(len(testing_data)*sample_percentage)\n",
    "label_tweet = zip(result,testing_data[:sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare distribution of predictions for training vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### REVISIT TO build stable clusters [the test set needs rebuilt?]\n",
    "###\n",
    "\n",
    "# results from training\n",
    "display(predictions.pred_cluster.value_counts(normalize=True))\n",
    "\n",
    "# results from test\n",
    "display(pd.Series(result).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_tweet[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top loaded terms\n",
    "We can now explore the clusters to understand their content a bit more. Develop the list of words top loaded into each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create contains to hold vocab    \n",
    "sorted_vocab_super_set = set()\n",
    "sorted_vocab_sub_sets = []\n",
    "\n",
    "# number of words to add to each subcluster\n",
    "top_n = 20\n",
    "\n",
    "\n",
    "for k in range(0,5):\n",
    "    # organize word loading indicies from largest to smallest\n",
    "    indices=[i for i in np.argsort(word_loadings_test1[k,:])[::-1]]    \n",
    "    # pull the vocab using the indicies\n",
    "    sorted_vocab_sub_sets.append(set([reduced_vocab.noun[i] for i in indices][:top_n]))\n",
    "    \n",
    "print len(sorted_vocab_sub_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(sorted_vocab_sub_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that top_n process worked; each set should have the top_n number of words\n",
    "for item in sorted_vocab_sub_sets:\n",
    "    print len(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortlist of terms above list above might suggest something about the meaning for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique terms per topic\n",
    "This list of terms represents words from the previous top loading list. So basically, we're just removing terms from the shortlist that also appear in other shortlists. This process seems to suggest topics in a human readable form as follows:\n",
    "\n",
    "- Cluster0: diet conversations\n",
    "- Cluster1: beer conversations\n",
    "- Cluster2: mealtime conversations (breakfast specific) \n",
    "- Cluster3: mealtime conversations (dinner specific)\n",
    "- Cluster4: mealtime conversations (lunch specific)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_vocab_sub_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up containers \n",
    "sorted_vocab_sub_sets_unique_values = []\n",
    "set_count = range(len(sorted_vocab_sub_sets))\n",
    "display(set_count)\n",
    "\n",
    "\n",
    "def update_set(list_of_lists,superset=set()):\n",
    "    \"\"\"\n",
    "    Flattens a list of lists and appends to the reults to a single set.\n",
    "    \"\"\"\n",
    "    for listN in list_of_lists:\n",
    "        superset.update(listN)\n",
    "    return superset\n",
    "\n",
    "for i in set_count:\n",
    "    # add new values to the set\n",
    "    superset = update_set([sorted_vocab_sub_sets[index] for index in set_count if i != index],set())\n",
    "    # add unique values to subtopic clusterID\n",
    "    sorted_vocab_sub_sets_unique_values.append( sorted_vocab_sub_sets[i] - superset )\n",
    "pp.pprint(sorted_vocab_sub_sets_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count of unique words (w/ top_n) per subcluster\n",
    "for item in sorted_vocab_sub_sets_unique_values:\n",
    "    print len(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic specific vocabulary\n",
    "The columns of the word loadings represents the vocabulary terms that are unique to each level  1 topic. Hence, each row counts the weight for the corresponding cluster terms. \n",
    "\n",
    "We can iterate through the columns and extract the index of the max word loading values. We consider the largest word loadings for each dimension to determine has the largest affect from that dimension. We will be able to see where each of the 1,109 vocabulary terms has the strongest pull in each cluster.\n",
    "\n",
    "We can then use the term and corresponding max loaded topic as a means to for a cluster specific vocabulary, which will vectorize tweets from this cluster. We will then apply kmeans again to create subtopics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (topics, number of terms)\n",
    "word_loadings_test1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_cluster_max={}\n",
    "with open('<filePath>','wb') as f:\n",
    "    # Iterate through the dimensions (each column is a word)\n",
    "    for col in range(word_loadings_test1.shape[1]):\n",
    "        # find the index of the row with the max word loading\n",
    "        topicID = np.argmax(word_loadings_test1[:,col])\n",
    "        # write to file the term's topicID that contains the max weight\n",
    "        max_val = np.amax(word_loadings_test1[:,col])\n",
    "        # drop terms that have no weight\n",
    "        if max_val > 0.0:\n",
    "            f.write(json.dumps(\n",
    "                    {\n",
    "                        reduced_vocab.noun[col]:\n",
    "                        [\n",
    "                            topicID # topicID (index)\n",
    "                            , max_val # max weight (value)\n",
    "                            , col                                 # column\n",
    "                        ]\n",
    "                    })+'\\n')\n",
    "            # build dictionary of the term's topicID that contains the max weight\n",
    "            term_cluster_max[reduced_vocab.noun[col]] = [\n",
    "                np.argmax(word_loadings_test1[:,col])\n",
    "                , np.amax(word_loadings_test1[:,col])\n",
    "                , col\n",
    "            ]\n",
    "        #f.write(json.dumps(term_cluster_max)+'\\n')\n",
    "#sorted_term_cluster_max = sorted(term_cluster_max, key=lambda k: k[0] )\n",
    "#sorted_term_cluster_max = sorted( term_cluster_max.items(), key=operator.itemgetter(1) )\n",
    "\n",
    "# sort by the dictionary by the cluster values\n",
    "sorted_term_cluster_max = sorted( term_cluster_max.items(), key = lambda k: k[1][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp.pprint(sorted_term_cluster_max[-10:-1])\n",
    "print len(sorted_term_cluster_max)\n",
    "display([item for item in sorted_term_cluster_max if item[0] == u'beer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual cluster specific vocabulary\n",
    "Now that we have each term's strongest topicID, the question remains: how do we explore it? We can plot the number of terms that map strongest to each subtopicID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster= {}\n",
    "# build strong term counts for each cluster\n",
    "for token,l in sorted_term_cluster_max:\n",
    "    cluster[l[0]] = cluster.get(l[0],0)+1\n",
    " \n",
    "cluster_df = pd.DataFrame({\"Cluster\":cluster.keys(),\"Term Count\":cluster.values()})\n",
    "display(cluster_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the total 1,109 words map strongest to topic 0 and topic 1 possibly because these are more general conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_df.plot(x=\"Cluster\",y=\"Term Count\",kind = 'bar',rot=0, title=\"Term Couns by Cluster ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build subtopics \n",
    "Now we'll take the labeled tweets and use the topic specific vocabulary to vectorize. From this vectorization, we can apply kmeans and find subtopics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_vocab = {ID:[] for ID in cluster.keys() }\n",
    "display(cluster_vocab)\n",
    "# We now invert the sorted_term_cluster_max dict to organize the vocabulary by topicID \n",
    "for token,l in sorted_term_cluster_max:\n",
    "    cluster_vocab[ l[0] ].append(token)\n",
    "cluster_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these subsets of vocabulary to vectorize the tweets in `X_train` and build the corresponding `sub toipc predictions` with kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(len(predictions))\n",
    "display(X_train.shape)\n",
    "display(len(training_data))\n",
    "\n",
    "labeled_tweets = pd.DataFrame({'clusterID':predictions.pred_cluster, 'tweet':training_data})\n",
    "labeled_tweets[:5]\n",
    "labeled_tweets[labeled_tweets.clusterID == 1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Review the lists of lists containing topic specific vocabulary\n",
    "cluster_vocab[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "k_subtopics = 5\n",
    "# create a dataframe containing the labeled tweets\n",
    "labeled_tweets = pd.DataFrame({'clusterID':predictions.pred_cluster\n",
    "                               , 'tweet':training_data \n",
    "                              })\n",
    "\n",
    "def create_subtopics_model(cluster_vocab, labeled_tweets, k_subtopics=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    centroids_per_cluster = {}\n",
    "    subtopic_dict = {}\n",
    "    for cluster,vocab in cluster_vocab.items():\n",
    "        print 'processing cluster{} using {}'.format(cluster, vocab)\n",
    "        # build vectorizer function based on the current cluster's vocab\n",
    "        subtopic_vectorizer = vectorize_(vocab)\n",
    "        \n",
    "        # apply the vectorizer to the current cluster's tweets\n",
    "        X_subtopics = subtopic_vectorizer.fit_transform(labeled_tweets[labeled_tweets.clusterID == cluster].tweet)\n",
    "        \n",
    "        # apply lsa\n",
    "        #lsa = make_pipeline(X_subtopics, Normalizer(copy=False))\n",
    "        #X_subtopics = lsa.fit_transform(X_subtopics)\n",
    "        \n",
    "        # build KMeans function \n",
    "        km = KMeans(n_clusters=k_subtopics\n",
    "                , init='k-means++'\n",
    "                , max_iter=100\n",
    "                #, n_init=10\n",
    "                , verbose=False)\n",
    "        \n",
    "        # apply kmeans to the current cluster's tweet set\n",
    "        km.fit(X_subtopics)\n",
    "        \n",
    "        # predcit subtopic clusters\n",
    "        pred=km.predict(X_subtopics)\n",
    "        print pred\n",
    "        # add results to dataframe\n",
    "        \n",
    "        subtopic_dict[cluster] = {'tweets':list(labeled_tweets[labeled_tweets.clusterID == cluster].tweet), 'subtopic_label':pred.tolist()}\n",
    "        #pred_df=pd.DataFrame(pred)\n",
    "        #pred_df.columns=['pred_cluster']\n",
    "        \n",
    "        # add centroids\n",
    "        centroids_per_cluster[cluster] = km.cluster_centers_\n",
    "        \n",
    "        # add inertia (currently not used as an output)\n",
    "        inertia = km.inertia_\n",
    "    return centroids_per_cluster , subtopic_dict\n",
    "\n",
    "subtopic_centroids_dict, labeled_tweets_dict = create_subtopics_model(cluster_vocab, labeled_tweets, k_subtopics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GREAT! So now we have 5 subtopics for each topic. We can see the distributions tweets categorized into the subtopics below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster,d in labeled_tweets_dict.items():\n",
    "    print cluster, collections.Counter(d['subtopic_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words per sub topic cluster\n",
    "Let's explore the top words for each sub topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "with open('subtopic_centroids_dict.json','wb') as f:\n",
    "    pickle.dump(subtopic_centroids_dict,f)\n",
    "    np.save(f,subtopic_centroids_dict)\n",
    "    \n",
    "with open('cluster_vocab.json','wb') as f:\n",
    "    pickle.dump(cluster_vocab,f)\n",
    "    np.save(f,cluster_vocab)\n",
    "    \n",
    "with open('labeled_tweets_dict.json','wb') as f:\n",
    "    pickle.dump(labeled_tweets_dict,f)\n",
    "    np.save(f,labeled_tweets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subtopic_centroids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_tweets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cluster_vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "### review alignment w/ original clusters\n",
    "###\n",
    "\n",
    "top_n = 50\n",
    "for cluster,weights in subtopic_centroids_dict.items():\n",
    "    print '\\n Cluster {}: '.format(cluster)\n",
    "    for i in range(weights.shape[0]):\n",
    "        print '  Top words for subtopic {}:'.format(i)\n",
    "        index_vals = np.array(np.argsort(weights[i])[::-1][:top_n])\n",
    "        print '    {}'.format([ cluster_vocab[cluster][x] for x in index_vals ])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Analysis by Topic and Subtopic\n",
    "Let's explore top tweets from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_tweets_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(labeled_tweets[:10])\n",
    "#[(k,v) for k,v in labeled_tweets_dict.items()][:1]\n",
    "subtopic_centroids_dict\n",
    "\n",
    "for level1Cluster,d in labeled_tweets_dict.items():\n",
    "    print zip(d['subtopic_label'],d['tweets'])\n",
    "\n",
    "\n",
    "\n",
    "def label_tweets(vectorizer, testing_data, sample_percentage=0.20):\n",
    "    \"\"\"\n",
    "    Label tweets using word loadings previously derived.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    sample_size = int(len(testing_data)*sample_percentage)\n",
    "    sample_tweets = testing_data[:sample_size]\n",
    "    for tweet in sample_tweets:\n",
    "        #sparse_array = vectorizer2.fit_transform([tweet])\n",
    "        sparse_array = vectorizer.fit_transform([tweet])\n",
    "        sparse_array_subtraction_abs = np.absolute(sparse_array - word_loadings)\n",
    "        sparse_array_subtraction_abs_sum = sparse_array_subtraction_abs.sum(axis=1)\n",
    "        result.append(np.argmin(sparse_array_subtraction_abs_sum))\n",
    "    return result\n",
    "\n",
    "#result = label_tweets(vectorizer, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argpartition(subtopic_centroids_dict[0][1],np.argmax(subtopic_centroids_dict[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([9, 4, 4, 3, 3, 9, 0, 4, 6, 0])\n",
    "\n",
    "ind1 = np.argsort(a)\n",
    "display(ind1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subtopic_centroids_dict[0]\n",
    "labeled_tweets_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\":[4,5,6],\"y\":['A','B','C'], 'bob':[0,0,0]})\n",
    "df[df['x']==4].bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_tweets_dict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'clusterID':predictions.pred_cluster, 'tweet':training_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head term_cluster_max.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[1],[1],[1]]).T\n",
    "b = np.array( [[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5]] )\n",
    "a[a>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notes from 2015-09-08:\n",
    "1.  Add details to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv files for aggregation of all topic models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('daily_2014.csv','rb') as d2014:\n",
    "    data = d2014.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(filepath_or_buffer ='daily_2014.csv'\n",
    "                ,header=None\n",
    "                ,names=['TS','tID-label-counter','count','ruleCount','seconds'])\n",
    "labels = x['tID-label-counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r'\\d+',\n",
    "for label in labels:\n",
    "    tID-label-counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
