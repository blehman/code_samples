{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad725ac8-1292-4634-9313-ace59d4e31eb",
   "metadata": {},
   "source": [
    "### Virtual Environment\n",
    "The env is available in the `tweet_topic_modeling_environment.yml` and was output in this way via command line:\n",
    "\n",
    "* !`conda env export --from-history > tweet_topic_modeling_environment.yml`.\n",
    "\n",
    "See [creating-an-environment-from-an-environment-yml-file](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file) on Conda's site to load the even. \n",
    "\n",
    "Side note: \n",
    "As I was adding packages to teh environment, I found it useful to add conda-forge to my channels\n",
    "* `conda config --add channels conda-forge`\n",
    "* `conda config --set channel_priority strict`\n",
    "  \n",
    "### kernel\n",
    "After activating the conda environment, make the kernel accessible (see [stackoverflow](https://stackoverflow.com/a/44786736)):\n",
    "* !`python -m ipykernel install --user --name tweet_topic_modeling --display-name \"tweet_topic_modeling\"`\n",
    "\n",
    "### IDE\n",
    "Jupyter Lab is used and the \"tweet_topic_modeling\" kernel is selected. \n",
    "\n",
    "* !`jupyter lab`\n",
    "\n",
    "### Data\n",
    "Post Elon, the site formerly known as Twitter no longer makes tweet data freely available. So I am using data from one of my topic modeling projects from 2015 (see my [topic-modeling-201](https://github.com/DrSkippy/Data-Science-45min-Intros/tree/master/topic-modeling-201) repo). The data is 5000 tweets using the search term \"golden retreiver\" that was freely snagged from Twitter's public API in 2015. \n",
    "### Pip\n",
    "\n",
    "I had to use `pip` to install a few libs that conda wouldn't give me:\n",
    "\n",
    "* `python -m pip install langid`\n",
    "* `python -m pip install langdetect`\n",
    "  \n",
    "### Method\n",
    "The goal here was to see what ChatGPT might recommend for the topic modeling I did in previously in [topic-modeling-201](https://github.com/DrSkippy/Data-Science-45min-Intros/tree/master/topic-modeling-201). So I initially started with this prompt:\n",
    "\n",
    "**promp1**: \"I have 5000 tweet's text in a python list from a 2015 version of Twitter's free public API using \"golden retriever\" as the search term. Can you split this list into a train and test set, train a topic model on the training set, then label the test set using the topic model?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25dde32d-ecb3-4a5e-8960-9769f8c88a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: tweet_topic_modeling\n",
      "channels:\n",
      "  - conda-forge\n",
      "  - defaults\n",
      "dependencies:\n",
      "  - python==3.11\n",
      "  - pandas\n",
      "  - jupyter\n",
      "  - matplotlib\n",
      "  - gensim\n",
      "  - nltk\n",
      "  - scikit-learn\n",
      "prefix: /Users/lehman/opt/anaconda3/envs/tweet_topic_modeling\n"
     ]
    }
   ],
   "source": [
    "!conda env export --from-history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461f81d-d7e5-4e50-be75-dd560f4eaa1f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de14bc31-7eca-471c-8441-c91f12563b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('data/tweet_text.pkl', 'rb') as file:\n",
    "    tweet_text = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941970a-5606-48ab-a833-d3cfc46fbc14",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820f33d-1519-4295-b87e-0ba50f54aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed116014-9258-4e1d-b5ce-314fd46d7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b8ee7-af5c-4643-8ea4-1c7b3bc2a6e9",
   "metadata": {},
   "source": [
    "### Intial Thoughts on the data\n",
    "1. I **think** the RT in the text was an optional and probably now outdated way of establishing that a tweet is actually a retweet; so I plan to remove it from the txt b/c I'm not concerned for these purposes if something is a retweet. \n",
    "2. In the first 10 tweets, 8 had links, but only 1 link took me to a non-deleted tweet from a non-suspended user.\n",
    "    * What should I do with links in general?\n",
    "        * I think that I will remove them to focus purely on the text. \n",
    "    * What should I do with text that links to deleted or suspended content?\n",
    "        * For the purpose of this topic modeling exercise, I'm not concerned that automated bots or humans made the content.\n",
    "3. The @ mentions should be removed to clean the text\n",
    "4. Punctuation could be removed to clean the text.\n",
    "5. Should we start with stemming? I'm keen to get to the root of the topics and want to make things like \"snuggler\", \"snuggles\", \"snuggling\" into \"snug\" rather than each being an individual topic. So I may just start here.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd5d46-afec-43f6-8056-ab979c7610a4",
   "metadata": {},
   "source": [
    "### ChatGPT Prompt Update\n",
    "**promp2**: \"I have text from 5000 tweets in a python list variable named \"tweet_text\".  The text was pulled from a 2015 version of Twitter's free public API using \"golden retriever\" as the search term. I'd like to remove urls, remove the word \"RT\", remove punctuation from the text, and remove common stop words. I'd also like to lowercase all of the text, and use stemming. Can you clean this data accordingly, split this list into a train and test set, train a topic model on the training set, and then label the test set using the topic model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf1af8-cd72-4bfb-b6bd-28d2dd2c5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c12964-fd67-4232-93be-e939a4041e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample list of 5000 tweet texts\n",
    "tweet_texts = tweet_text  # Your list of tweet texts here\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove \"RT\"\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization and lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "processed_texts = [preprocess_text(text) for text in tweet_texts]\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts = train_test_split(processed_texts, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize again for training LDA\n",
    "train_tokens = [text.split() for text in train_texts]\n",
    "\n",
    "# Create dictionary and corpus for topic modeling\n",
    "dictionary = corpora.Dictionary(train_tokens)\n",
    "train_corpus = [dictionary.doc2bow(tokens) for tokens in train_tokens]\n",
    "\n",
    "# Perform topic modeling using LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=train_corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            passes=10,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# Label test set using the trained topic model\n",
    "test_corpus = [dictionary.doc2bow(text.split()) for text in test_texts]\n",
    "\n",
    "# Predict topics for test set\n",
    "test_topic_labels = []\n",
    "for doc in test_corpus:\n",
    "    topic_distribution = lda_model.get_document_topics(doc)\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    test_topic_labels.append(dominant_topic)\n",
    "\n",
    "# Print example of test tweet with its topic label\n",
    "for i in range(5):  # Print labels for the first 5 test tweets\n",
    "    print(\"Test tweet:\", test_texts[i])\n",
    "    print(\"Topic label:\", test_topic_labels[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea22e7-ec29-48d4-b324-a24d18c5ce29",
   "metadata": {},
   "source": [
    "### Thoughts on topics\n",
    "1. I'll remove tweets that are not unique and see what sort of dataset I have left.\n",
    "2. I'm going to want to understand what the topics mean as well.\n",
    "\n",
    "### ChatGPT Prompt Update\n",
    "\n",
    "I have text from 5000 tweets in a python list variable named \"tweet_text\".  The text was pulled from a 2015 version of Twitter's free public API using \"golden retriever\" as the search term. I'd like to remove urls, remove the words that start with @ (these are the user mentions), remove the word \"RT\", remove punctuation from the text, and remove common stop words. I'd also like to lowercase all of the text, and use stemming. I would then like to unique the list of tweets so we only train on unique tweets. Can you clean this data accordingly, split this list into a train and test set, train a topic model on the training set, and then label the test set using the topic model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a200bef-3c9e-4b14-aee2-1790fbd2556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample list of 5000 tweet texts\n",
    "tweet_texts = tweet_text  # Your list of tweet texts here\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove \"RT\"\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization and lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Clean and unique the tweet texts\n",
    "unique_tweet_texts = list(set([preprocess_text(text) for text in tweet_texts]))\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts = train_test_split(unique_tweet_texts, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize again for training LDA\n",
    "train_tokens = [text.split() for text in train_texts]\n",
    "\n",
    "# Create dictionary and corpus for topic modeling\n",
    "dictionary = corpora.Dictionary(train_tokens)\n",
    "train_corpus = [dictionary.doc2bow(tokens) for tokens in train_tokens]\n",
    "\n",
    "# Perform topic modeling using LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=train_corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            passes=10,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# Label test set using the trained topic model\n",
    "test_corpus = [dictionary.doc2bow(text.split()) for text in test_texts]\n",
    "\n",
    "# Predict topics for test set\n",
    "test_topic_labels = []\n",
    "for doc in test_corpus:\n",
    "    topic_distribution = lda_model.get_document_topics(doc)\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    test_topic_labels.append(dominant_topic)\n",
    "\n",
    "# Print example of test tweet with its topic label\n",
    "for i in range(5):  # Print labels for the first 5 test tweets\n",
    "    print(\"Test tweet:\", test_texts[i])\n",
    "    print(\"Topic label:\", test_topic_labels[i])\n",
    "    print(\"\")\n",
    "\n",
    "# Print topics and their meanings\n",
    "print(\"Topics and their meanings:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic {}: {}\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb6809-566a-4ba2-835f-964052b933d5",
   "metadata": {},
   "source": [
    "### Thoughts on output \n",
    "1. What size is our dataset including train/test?\n",
    "   * We went from 5000 tweets to 300 unique tweets.\n",
    "1. Should I try an alternative method?\n",
    "   * I'm going to explore this idea with ChatGPT in a few prompts, but BERT might be fun to try.\n",
    "1. Are the topics very distinct? Meaning, are the probabilities for the tweets predicting one clear winner or are the probabilities roughly equal across all topics?\n",
    "1. The meaning of the topics is opaque. How can we visualize the results?\n",
    "   * I recall pyLDAvis was cool, but I'm keen to explore new ways so I'll also ask ChatGPT for recs.\n",
    "1. Language? Do we need to remove tweets that are none english?\n",
    "   * For my own purposes, choosing English or Spanish would make the output more understandable to me. I'll choose english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64714a60-adce-41db-80c6-52eca5f6bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"------------------------------------------------\\n\n",
    "initial_tweets:  {initial_tweet_count}\n",
    "unique tweets:   {unique_tweet_count}\n",
    "training tweets: {train_set_size}\n",
    "test tweets:     {test_set_size}\\n\n",
    "total topics:    {total_topics}\n",
    "------------------------------------------------\\n\"\"\".format(initial_tweet_count=len(tweet_text)\n",
    "                                                             , unique_tweet_count=len(unique_tweet_texts)\n",
    "                                                             , train_set_size=len(train_texts)\n",
    "                                                             , test_set_size=len(test_texts)\n",
    "                                                             , total_topics=len(lda_model.print_topics(-1))\n",
    "                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d536e-f90b-4405-8bd6-b64aa811ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 5 tweets with its topic label and probability\n",
    "for i, doc in enumerate(random.sample(test_corpus,5)):\n",
    "    print(\"Test tweet:\", test_texts[i])\n",
    "    topic_distribution = lda_model.get_document_topics(doc)\n",
    "    for topic, prob in topic_distribution:\n",
    "        print(\"Topic label:\", topic, \"Probability:\", prob)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830c511-ddad-4619-a7d5-778a7cbc5b45",
   "metadata": {},
   "source": [
    "### choose lang lib\n",
    "I want the library that leaves me with the most tweets; I'm not going to stress about accuracy at this point, but I just want something to removing tweets not in english. \n",
    "* I picked langid b/c it ran faster and left me with more tweets\n",
    "* The spot check didn't surface anything in either group that seems suspicous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56266054-4ebd-4c7c-80f5-7e8809059281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# Filter out non-English tweets\n",
    "english_tweets = []\n",
    "for tweet in tweet_text:\n",
    "    try:\n",
    "        if detect(tweet) == 'en':\n",
    "            english_tweets.append(tweet)\n",
    "    except:\n",
    "        pass  # Skip tweets that raise exceptions (e.g., empty tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e17a1-ffb1-44d4-946a-11fdecbfb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(english_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d93c2-ab4b-4b38-b8bb-facb7bab805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample(english_tweets, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994cd33-6a84-46fb-a30e-786f5fea186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "# Filter out non-English tweets\n",
    "english_tweets_langid = []\n",
    "for tweet in tweet_text:\n",
    "    lang, _ = langid.classify(tweet)\n",
    "    if lang == 'en':\n",
    "        english_tweets_langid.append(tweet)\n",
    "\n",
    "print(len(english_tweets_langid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbf209-a26e-4402-be23-446abe8ea25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(english_tweets_langid, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07319f8e-1355-4ff4-99a1-19083d28880f",
   "metadata": {},
   "source": [
    "### Run Model with updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02d465-1999-43fd-8177-dabd03c96b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import gensim\n",
    "import langid\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample list of 5000 tweet texts\n",
    "tweet_texts = tweet_text  # Your list of tweet texts here\n",
    "\n",
    "# Keep only tweets classified as english\n",
    "english_tweet_texts = []\n",
    "for tweet in tweet_texts:\n",
    "    lang, _ = langid.classify(tweet)\n",
    "    if lang == 'en':\n",
    "        english_tweet_texts.append(tweet)\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove \"RT\"\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization and lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Remove stopwords and specific terms\n",
    "    #words = [word for word in words if word not in stop_words and word not in ['golden', 'retriev']]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Clean and unique the tweet texts\n",
    "unique_english_tweet_texts = list(set([preprocess_text(text) for text in english_tweet_texts]))\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts = train_test_split(unique_english_tweet_texts, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize again for training LDA\n",
    "train_tokens = [text.split() for text in train_texts]\n",
    "\n",
    "# Create dictionary and corpus for topic modeling\n",
    "dictionary = corpora.Dictionary(train_tokens)\n",
    "train_corpus = [dictionary.doc2bow(tokens) for tokens in train_tokens]\n",
    "\n",
    "# Perform topic modeling using LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=train_corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            passes=10,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# Label test set using the trained topic model\n",
    "test_corpus = [dictionary.doc2bow(text.split()) for text in test_texts]\n",
    "\n",
    "# Predict topics for test set\n",
    "test_topic_labels = []\n",
    "for doc in test_corpus:\n",
    "    topic_distribution = lda_model.get_document_topics(doc)\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    test_topic_labels.append(dominant_topic)\n",
    "\n",
    "# Print example of test tweet with its topic label\n",
    "for i in range(5):  # Print labels for the first 5 test tweets\n",
    "    print(\"Test tweet:\", test_texts[i])\n",
    "    print(\"Topic label:\", test_topic_labels[i])\n",
    "    print(\"\")\n",
    "\n",
    "# Print topics and their meanings\n",
    "print(\"Topics and their meanings:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic {}: {}\".format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d42ee-331d-42d0-9e40-7b2337162288",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "* My aim here is to explore the topics distinct nature and meaning in a human readable way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24761629-bbb7-42e2-811d-07c74b171957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate word clouds for each topic\n",
    "for idx, topic in lda_model.show_topics(formatted=False):\n",
    "    word_freq = {word: freq for word, freq in topic}\n",
    "    wordcloud = WordCloud(background_color='white').generate_from_frequencies(word_freq)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title('Topic {}'.format(idx))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create bar plots for the most frequent terms in each topic\n",
    "for idx, topic in lda_model.show_topics(formatted=False):\n",
    "    terms, freqs = zip(*topic)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(range(len(terms)), freqs, align='center', color='skyblue')\n",
    "    plt.yticks(range(len(terms)), terms)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title('Topic {}'.format(idx))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c17ced-8752-4908-9e7f-3cf028b52747",
   "metadata": {},
   "source": [
    "### Thoughts on Visuals\n",
    "1. 'Golden' and 'Retriv' may not need to be in the visual; I'll add code to remove it but comment it out for now.\n",
    "2. These topics may fit into a customer journey; Golden Retriever being the product taken from awereness to aquisition to loyalty, etc.\n",
    "3. These visuals don't show the overlap in similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d6982-ecc7-4021-8c44-2b7713e9eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# # Extract topics and associated terms from the LDA model\n",
    "# topics_terms = lda_model.show_topics(formatted=False)\n",
    "\n",
    "# # Extract topic-term distributions for each topic\n",
    "# topic_terms = {idx: [term for term, _ in topic] for idx, topic in topics_terms}\n",
    "\n",
    "# # Create bar plots for each topic showing the top terms\n",
    "# fig_terms = go.Figure()\n",
    "# for idx, terms in topic_terms.items():\n",
    "#     fig_terms.add_trace(go.Bar(x=terms, y=[1]*len(terms), name=f'Topic {idx}', orientation='h'))\n",
    "\n",
    "# fig_terms.update_layout(title='Top Terms in Each Topic', barmode='stack', xaxis_title='Term', yaxis_title='Topic')\n",
    "# fig_terms.show()\n",
    "\n",
    "# # Extract topic-document distributions for the test set\n",
    "# topic_distribution_test = [lda_model.get_document_topics(doc) for doc in test_corpus]\n",
    "\n",
    "# # Create stacked bar plot showing topic distribution in test documents\n",
    "# fig_distribution = go.Figure()\n",
    "# for idx, topic_dist in enumerate(topic_distribution_test):\n",
    "#     probs = [prob for _, prob in topic_dist]\n",
    "#     fig_distribution.add_trace(go.Bar(x=[f'Topic {i}' for i in range(len(probs))], y=probs, name=f'Document {idx}'))\n",
    "\n",
    "# fig_distribution.update_layout(title='Topic Distribution in Test Documents', barmode='stack', xaxis_title='Topic', yaxis_title='Probability')\n",
    "# fig_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa8d6f-8d2a-47c5-8110-91775fc783bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = {}\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    terms = [term.split(\"*\")[1].strip().strip('\"') for term in topic.split(\"+\")]\n",
    "    topic_terms[f\"Topic {idx}\"] = terms\n",
    "\n",
    "# Print topic-term distributions\n",
    "for topic, terms in topic_terms.items():\n",
    "    print(f\"{topic}: {terms}\")\n",
    "\n",
    "topic_terms_with_counts = [{ 'topic': topic, 'count': len(terms) } for topic, terms in topic_terms.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4950f-a27e-4c67-be6a-9c795209a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import gensim\n",
    "import langid\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "#from IPython.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# Sample list of 5000 tweet texts\n",
    "tweet_texts = tweet_text  # Your list of tweet texts here\n",
    "\n",
    "# Keep only tweets classified as english\n",
    "english_tweet_texts = []\n",
    "for tweet in tweet_texts:\n",
    "    lang, _ = langid.classify(tweet)\n",
    "    if lang == 'en':\n",
    "        english_tweet_texts.append(tweet)\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove \"RT\"\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization and lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Remove stopwords and specific terms\n",
    "    #words = [word for word in words if word not in stop_words and word not in ['golden', 'retriev']]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Clean and unique the tweet texts\n",
    "unique_english_tweet_texts = list(set([preprocess_text(text) for text in english_tweet_texts]))\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts = train_test_split(unique_english_tweet_texts, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize again for training LDA\n",
    "train_tokens = [text.split() for text in train_texts]\n",
    "\n",
    "# Create dictionary and corpus for topic modeling\n",
    "dictionary = corpora.Dictionary(train_tokens)\n",
    "train_corpus = [dictionary.doc2bow(tokens) for tokens in train_tokens]\n",
    "\n",
    "# Perform topic modeling using LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=train_corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            passes=10,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# Prepare the visualization data\n",
    "lda_display = gensimvis.prepare(lda_model, train_corpus, dictionary, sort_topics=False)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b426e9d-ac69-42bf-8618-8a23b6c63f44",
   "metadata": {},
   "source": [
    "### pyLDAvis Thoughts\n",
    "1. a few of the clusters seem to overlap on the toipc map; run LDA a few more times to see if randomness changes the positions. \n",
    "2. I'm not seeing a rich \"theme\" in the topic content. I'm going to try to use BERT embeddings to represent the text data and then apply LDA to find topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767dcd4-39f8-4226-93eb-14476021396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import gensim\n",
    "import langid\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Sample list of 5000 tweet texts\n",
    "tweet_texts = tweet_text  # Your list of tweet texts here\n",
    "\n",
    "# Keep only tweets classified as English\n",
    "english_tweet_texts = [tweet for tweet in tweet_texts if langid.classify(tweet)[0] == 'en']\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove \"RT\"\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization and lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Clean and unique the tweet texts\n",
    "unique_english_tweet_texts = list(set([preprocess_text(text) for text in english_tweet_texts]))\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Obtain BERT embeddings for each tweet\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "bert_embeddings = []\n",
    "\n",
    "for text in unique_english_tweet_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1).squeeze().numpy()\n",
    "    bert_embeddings.append(embeddings)\n",
    "\n",
    "# Convert BERT embeddings to a format suitable for LDA\n",
    "# For example, you can concatenate the embeddings for each tweet into a single vector\n",
    "lda_inputs = [emb.flatten() for emb in bert_embeddings]\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_inputs, test_inputs = train_test_split(lda_inputs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform topic modeling using LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=train_inputs,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            passes=10,\n",
    "                                            per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4df7a-2883-4812-a0f0-3104311e701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Visualization 1: Word Clouds for each topic\n",
    "def visualize_wordclouds(lda_model):\n",
    "    topics = lda_model.show_topics(num_topics=-1, formatted=False)\n",
    "    for topic_id, words in topics:\n",
    "        word_freq = {word: freq for word, freq in words}\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Topic {topic_id}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualization 2: pyLDAvis\n",
    "def visualize_pyldavis(lda_model, corpus, dictionary):\n",
    "    lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
    "    pyLDAvis.display(lda_display)\n",
    "\n",
    "# Visualization 3: Topic Coherence\n",
    "def compute_coherence(lda_model, corpus, dictionary):\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=train_tokens, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f'Topic Coherence Score: {coherence_lda}')\n",
    "\n",
    "# Visualize word clouds for each topic\n",
    "visualize_wordclouds(lda_model)\n",
    "\n",
    "# Visualize topics using pyLDAvis\n",
    "visualize_pyldavis(lda_model, train_corpus, dictionary)\n",
    "\n",
    "# Compute and print topic coherence score\n",
    "compute_coherence(lda_model, train_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325ec349-10c3-40ad-86ad-4beb9f366bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fef840-8d13-4e8f-af67-3d82fa00bb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Time taken for data preprocessing: 3.6213109493255615 seconds\n",
      "Time taken for text preprocessing: 0.5432548522949219 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import gensim\n",
    "import langid\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Sample list of 5000 tweet texts\n",
    "tweet_texts = tweet_text  # Your list of tweet texts here\n",
    "\n",
    "# Keep only tweets classified as English\n",
    "english_tweet_texts = [tweet for tweet in tweet_texts if langid.classify(tweet)[0] == 'en']\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for data preprocessing:\", end - start, \"seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove \"RT\"\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization and lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Clean and unique the tweet texts\n",
    "unique_english_tweet_texts = list(set([preprocess_text(text) for text in english_tweet_texts]))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for text preprocessing:\", end - start, \"seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Obtain BERT embeddings for each tweet\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "bert_embeddings = []\n",
    "\n",
    "for text in unique_english_tweet_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1).squeeze().numpy()\n",
    "    bert_embeddings.append(embeddings)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for BERT embeddings:\", end - start, \"seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Convert BERT embeddings to a format suitable for LDA\n",
    "# For example, you can concatenate the embeddings for each tweet into a single vector\n",
    "lda_inputs = [emb.flatten() for emb in bert_embeddings]\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for data conversion for LDA:\", end - start, \"seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_inputs, test_inputs = train_test_split(lda_inputs, test_size=0.2, random_state=42)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for data splitting:\", end - start, \"seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Perform topic modeling using LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=train_inputs,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            passes=10,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for LDA model training:\", end - start, \"seconds\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Visualization 1: Word Clouds for each topic\n",
    "def visualize_wordclouds(lda_model):\n",
    "    topics = lda_model.show_topics(num_topics=-1, formatted=False)\n",
    "    for topic_id, words in topics:\n",
    "        word_freq = {word: freq for word, freq in words}\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Topic {topic_id}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualization 2: pyLDAvis\n",
    "def visualize_pyldavis(lda_model, corpus, dictionary):\n",
    "    lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
    "    pyLDAvis.display(lda_display)\n",
    "\n",
    "# Visualization 3: Topic Coherence\n",
    "def compute_coherence(lda_model, corpus, dictionary):\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=train_tokens, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f'Topic Coherence Score: {coherence_lda}')\n",
    "\n",
    "# Visualize word clouds for each topic\n",
    "visualize_wordclouds(lda_model)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for word cloud visualization:\", end - start, \"seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Visualize topics using pyLDAvis\n",
    "visualize_pyldavis(lda_model, train_corpus, dictionary)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for pyLDAvis visualization:\", end - start, \"seconds\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Compute and print topic coherence score\n",
    "compute_coherence(lda_model, train_corpus, dictionary)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken for coherence score computation:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af8425-2c0b-4500-a28f-71d67ba842aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet_topic_modeling",
   "language": "python",
   "name": "tweet_topic_modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
